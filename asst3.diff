This asst3 patch generated by /import/adams/1/cs3231/bin/cs3231_generate_diff for z5059988 on Sun  3 Jun 17:49:07 AEST 2018
diff --unidirectional-new-file -EdbwBr -u -X /home/cs3231/assigns/asst3/src/../diffex /home/cs3231/assigns/asst3/src/design.txt /import/ravel/3/z5059988/cs3231/asst3-src/design.txt
--- design.txt	2018-02-24 13:25:22.000000000 +1100
+++ design.txt	2018-06-03 17:46:35.199119043 +1000
@@ -1 +1,94 @@
-Place your design document here.
+COMP3231 Assignment 3 Design Document
+
+##############################
+Basic Issues in the Assignment
+##############################
+
+P: We needed a method of keeping track of which sections of physical memory
+were used or free.
+
+S: We implemented a frame table as a linked list of entries, with a head
+pointer referencing the entry for the next free frame available for allocation.
+Each entry tracks a physical frame number as well as a pointer to the entry
+tracking the next free frame, with the last entry in the list pointing to NULL.
+
+P: We needed a way to allocate and free frames in physical memory, both before
+and after the initialisation of our frame table.
+
+S: To account for the allocation of pages prior to the initialisation of our
+frame table, we use the function ram_stealmem(). However once our frame
+table is initialised, all requests to allocate pages use the entry pointed to
+by our free frame linked list. If more than one page is requested or the
+available frames are exhausted, a zero value is returned.
+
+P: Our frame table must be able to handle concurrent modification to prevent
+race conditions (e.g. multiple alloc_kpages() calls returning the same frame).
+
+S: To implement concurrency control, we first identified the critical sections
+of alloc_kpages() and free_kpages(), then used a spinlock to restrict access.
+
+P: We needed a means of providing virtual memory translation for user programs.
+
+S: We implemented a hashed page table (HPT) shared by all processes and a TLB
+refill handler (vm_fault()) which is called when we encounter a TLB miss.
+The refill handler either searches for an existing page table entry that maps
+to the correct address, or lazy loads a new page table entry if none is found.
+We store the virtual address and physical address for the mapping in the format
+used by the TLB (entryhi/entrylo) and also store the process ID. We also store
+a `next` pointer for collision resolution (see next problem).
+
+P: We needed a method to resolve hash collisions in our HPT.
+
+S: For our HPT, we used linear probing with internal chaining to resolve hash
+collisions. On hash collisions, we perform a linear search of our page table
+until we find the first available empty slot. We then insert our page into the
+slot and update the `next` pointer of the last entry in the existing collision
+chain to point to the newly-inserted page.
+
+P: We needed a way of providing synchronisation for the hashed page table.
+
+S: Since the HPT is a global data structure shared between processes, we needed
+to ensure that if one process inserts an entry into the HPT, other processes
+do not overwrite it (or see an intermediate partially-initialized HPT entry).
+To handle this, we used a single lock called `hpt_lock`, acquired and released
+before accessing the HPT entries.
+
+P: We needed a way to create and track the address space for each process.
+
+S: Our address space is designated by the list of regions associated with it
+(including a fixed-size stack region) and the total number of regions (used for
+debugging and corruption checks). Region specifications include their base
+virtual address, the number of pages and whether the region is
+readable/writeable. Since the regions form a linked list, we also have a
+pointer to the next region.
+
+P: We needed a way to track the read/write permissions for each region in an
+address space at various stages during the process's lifetime.
+
+S: In our region struct, we keep track of three flags: `readable`, `writeable`
+(temporary write permissions) and `can_write` ("persistent" write permissions).
+We use two flags to track write permissions since all regions need to be
+writeable when being loaded by the ELF, however once loading is finished,
+regions that were previously read-only need to be reverted, while regions that
+previously also had write permissions needed to retain their status. As such,
+in as_prepare_load(), we set `writeable` to be true for every region,
+then in as_complete_load() we revert each region's permissions to their prior
+state. The `can_write` flag enables this by storing the "real" write permission
+for the region.
+
+#######################
+Summary of File Changes
+#######################
+
+kern/include/vm.h: contains definitions of our frame table and hashed page
+table entries
+
+kern/vm/frametable.c: contains implementations of our frame table manipulation
+functions (alloc_kpages/free_kpages)
+
+kern/vm/vm.c: contains implementations of our virtual memory functions
+
+kern/include/addrspace.h: contains definitions of our address space and region
+data structures
+
+kern/vm/addrspace.c: contains implementations of our address space functions
\ No newline at end of file
Only in /home/cs3231/assigns/asst3/src/kern/compile: .keep_me
diff --unidirectional-new-file -EdbwBr -u -X /home/cs3231/assigns/asst3/src/../diffex /home/cs3231/assigns/asst3/src/kern/include/addrspace.h /import/ravel/3/z5059988/cs3231/asst3-src/kern/include/addrspace.h
--- kern/include/addrspace.h	2018-02-24 13:25:22.000000000 +1100
+++ kern/include/addrspace.h	2018-06-03 17:46:35.203119045 +1000
@@ -38,8 +37,18 @@
 #include <vm.h>
 #include "opt-dumbvm.h"
 
+#define NUM_STACK_PAGES 16 /* fixed-size stack size */
+
 struct vnode;
 
+/* region specification */
+struct region {
+	vaddr_t vbase; /* start of region */
+	size_t npages; /* npages in region */
+	bool readable, writeable; /* whether the region is readable/writeable */
+	bool can_write; /* real write permissions - unlike writeable, this should not change */
+	struct region *next; /* pntr to next region */
+};
 
 /*
  * Address space - data structure associated with the virtual memory
@@ -47,9 +56,9 @@
  *
  * You write this.
  */
-
 struct addrspace {
 #if OPT_DUMBVM
+	/* dumbvm.c members */
         vaddr_t as_vbase1;
         paddr_t as_pbase1;
         size_t as_npages1;
@@ -58,10 +67,20 @@
         size_t as_npages2;
         paddr_t as_stackpbase;
 #else
-        /* Put stuff here for your VM system */
+	/* addrspace.c members */
+	uint32_t nregions;
+	struct region *region_list;
 #endif
 };
 
+/* function prototypes for hashed page table helpers */
+uint32_t hpt_hash(struct addrspace *as, vaddr_t faultaddr);
+int insert_ptable_entry(struct addrspace *as, vaddr_t vaddr, int writeable, bool write_tlb);
+void make_page_read_only(vaddr_t vaddr);
+ptable_entry search_ptable(struct addrspace *as, vaddr_t vaddr, ptable_entry *prev);
+void free_region(struct addrspace *as, vaddr_t vaddr, uint32_t npages);
+int copy_region(struct region *reg, struct addrspace *old, struct addrspace *newas);
+
 /*
  * Functions in addrspace.c:
  *
@@ -108,12 +126,8 @@
 void              as_activate(void);
 void              as_deactivate(void);
 void              as_destroy(struct addrspace *);
-
-int               as_define_region(struct addrspace *as,
-                                   vaddr_t vaddr, size_t sz,
-                                   int readable,
-                                   int writeable,
-                                   int executable);
+int as_define_region(struct addrspace *as, vaddr_t vaddr, size_t sz,
+	int readable, int writeable, int executable);
 int               as_prepare_load(struct addrspace *as);
 int               as_complete_load(struct addrspace *as);
 int               as_define_stack(struct addrspace *as, vaddr_t *initstackptr);
diff --unidirectional-new-file -EdbwBr -u -X /home/cs3231/assigns/asst3/src/../diffex /home/cs3231/assigns/asst3/src/kern/include/vm.h /import/ravel/3/z5059988/cs3231/asst3-src/kern/include/vm.h
--- kern/include/vm.h	2018-02-24 13:25:22.000000000 +1100
+++ kern/include/vm.h	2018-06-03 17:19:49.006217585 +1000
@@ -27,17 +27,37 @@
  * SUCH DAMAGE.
  */
 
+#include <machine/vm.h>
+
 #ifndef _VM_H_
 #define _VM_H_
 
 /*
  * VM system-related definitions.
- *
- * You'll probably want to add stuff here.
  */
 
+#define PAGE_BITS 12 /* this was not defined anywhere */
 
-#include <machine/vm.h>
+/* frame table entry */
+typedef struct frame_table_entry *ftable_entry;
+struct frame_table_entry {
+	uint32_t addr : 20; /* physical frame number */
+	uint32_t padding : 12; /* unused */
+	ftable_entry next; /* next free frame */
+};
+
+extern struct frame_table_entry *ftable;
+
+/* hashed page table entry */
+typedef struct page_table_entry *ptable_entry;
+struct page_table_entry {
+	pid_t pid; /* current process id */
+	uint32_t entryhi;
+	uint32_t entrylo;
+	ptable_entry next; /* internal chaining for collisions */
+};
+
+extern struct page_table_entry *ptable;
 
 /* Fault-type arguments to vm_fault() */
 #define VM_FAULT_READ        0    /* A read was attempted */
diff --unidirectional-new-file -EdbwBr -u -X /home/cs3231/assigns/asst3/src/../diffex /home/cs3231/assigns/asst3/src/kern/main/main.c /import/ravel/3/z5059988/cs3231/asst3-src/kern/main/main.c
--- kern/main/main.c	2018-02-24 13:25:22.000000000 +1100
+++ kern/main/main.c	2018-05-17 21:41:31.000000000 +1000
@@ -103,7 +103,7 @@
 	kprintf("%s", harvard_copyright);
 	kprintf("\n");
 
-	kprintf("Put-your-group-name-here's system version %s (%s #%d)\n",
+	kprintf("osprj175's system version %s (%s #%d)\n",
 		GROUP_VERSION, buildconfig, buildversion);
 	kprintf("\n");
 
diff --unidirectional-new-file -EdbwBr -u -X /home/cs3231/assigns/asst3/src/../diffex /home/cs3231/assigns/asst3/src/kern/vm/addrspace.c /import/ravel/3/z5059988/cs3231/asst3-src/kern/vm/addrspace.c
--- kern/vm/addrspace.c	2018-02-24 13:25:22.000000000 +1100
+++ kern/vm/addrspace.c	2018-06-03 17:46:35.207119047 +1000
@@ -38,153 +38,171 @@
 #include <vm.h>
 #include <proc.h>
 
-/*
- * Note! If OPT_DUMBVM is set, as is the case until you start the VM
- * assignment, this file is not compiled or linked or in any way
- * used. The cheesy hack versions in dumbvm.c are used instead.
- *
- * UNSW: If you use ASST3 config as required, then this file forms
- * part of the VM subsystem.
- *
- */
-
-struct addrspace *
-as_create(void)
-{
-        struct addrspace *as;
-
-        as = kmalloc(sizeof(struct addrspace));
-        if (as == NULL) {
-                return NULL;
-        }
+/* create a new addrspace */
+struct addrspace *as_create(void) {
+	struct addrspace *as = kmalloc(sizeof(struct addrspace));
+	if (as == NULL) return NULL;
 
-        /*
-         * Initialize as needed.
-         */
+	/* initialize as needed */
+	as->nregions = 0;
+	as->region_list = NULL;
 
         return as;
 }
 
-int
-as_copy(struct addrspace *old, struct addrspace **ret)
-{
-        struct addrspace *newas;
+/* copy an addrspace */
+int as_copy(struct addrspace *old, struct addrspace **ret) {
+	struct addrspace *newas = as_create();
+	if (newas == NULL) return ENOMEM;
 
-        newas = as_create();
-        if (newas==NULL) {
-                return ENOMEM;
+	/* copy over the region list */
+	struct region *curr;
+	for (curr = old->region_list; curr != NULL; curr = curr->next) {
+		int ret = as_define_region(newas, curr->vbase, curr->npages * PAGE_SIZE, curr->readable, curr->writeable, true);
+		if (ret) {
+			/* error in as_define_region() */
+			as_destroy(newas);
+			return ret;
         }
+	}
+	KASSERT(newas->nregions == old->nregions);
 
-        /*
-         * Write this.
-         */
-
-        (void)old;
+	/* copy region data from the old address space */
+	for (curr = newas->region_list; curr != NULL; curr = curr->next) {
+		int ret = copy_region(curr, old, newas);
+		if (ret) {
+			/* error in copy_region() */
+			as_destroy(newas);
+			return ret;
+		}
+	}
 
         *ret = newas;
+	as_activate();
         return 0;
 }
 
-void
-as_destroy(struct addrspace *as)
-{
-        /*
-         * Clean up as needed.
-         */
+/* free memory associated with an addrspace and release pages and frames */
+void as_destroy(struct addrspace *as) {
+	/* iterate through as->region_list and free each region */
+	struct region *curr = as->region_list;
+	while (curr != NULL) {
+		free_region(as, curr->vbase, curr->npages); /* free pages & frames */
+
+		/* free memory used by region struct */
+		struct region *to_free = curr;
+		curr = curr->next;
+		kfree(to_free);
+	}
 
         kfree(as);
+	as_activate();
 }
 
-void
-as_activate(void)
-{
-        struct addrspace *as;
+/* invalidate all tlb entries */
+void as_activate(void) {
+	struct addrspace *as = proc_getas();
+	if (as == NULL) return;
 
-        as = proc_getas();
-        if (as == NULL) {
-                /*
-                 * Kernel thread without an address space; leave the
-                 * prior address space in place.
-                 */
-                return;
-        }
+	/* disable interrupts on this cpu while frobbing the tlb */
+	int spl = splhigh();
 
-        /*
-         * Write this.
-         */
+	/* invalidate all tlb entries */
+	for (int i = 0; i < NUM_TLB; i++) {
+		tlb_write(TLBHI_INVALID(i), TLBLO_INVALID(), i);
+	}
+	splx(spl);
 }
 
-void
-as_deactivate(void)
-{
-        /*
-         * Write this. For many designs it won't need to actually do
-         * anything. See proc.c for an explanation of why it (might)
-         * be needed.
-         */
+/* invalidate all tlb entries just in case */
+void as_deactivate(void) {
+	as_activate();
 }
 
 /*
- * Set up a segment at virtual address VADDR of size MEMSIZE. The
- * segment in memory extends from VADDR up to (but not including)
- * VADDR+MEMSIZE.
- *
- * The READABLE, WRITEABLE, and EXECUTABLE flags are set if read,
- * write, or execute permission should be set on the segment. At the
- * moment, these are ignored. When you write the VM system, you may
- * want to implement them.
- */
-int
-as_define_region(struct addrspace *as, vaddr_t vaddr, size_t memsize,
-                 int readable, int writeable, int executable)
-{
-        /*
-         * Write this.
+ * Set up a segment at virtual address vaddr of size memsize. The
+ * segment in memory extends from vaddr up to (but not including)
+ * vaddr + memsize.
          */
+int as_define_region(struct addrspace *as, vaddr_t vaddr, size_t memsize,
+int readable, int writeable, int executable) {
+	(void) executable; /* unused */
 
-        (void)as;
-        (void)vaddr;
-        (void)memsize;
-        (void)readable;
-        (void)writeable;
-        (void)executable;
-        return ENOSYS; /* Unimplemented */
-}
+	/* allocate space for new region and set up its fields */
+	struct region *new_region = kmalloc(sizeof(struct region));
+	if (new_region == NULL) return ENOMEM;
+	size_t npages;
 
-int
-as_prepare_load(struct addrspace *as)
-{
-        /*
-         * Write this.
-         */
+	/* align the region base and length */
+	memsize += vaddr & ~(vaddr_t)PAGE_FRAME;
+	vaddr &= PAGE_FRAME;
+	memsize = (memsize + PAGE_SIZE - 1) & PAGE_FRAME;
+
+	npages = memsize / PAGE_SIZE;
+	KASSERT(npages != 0);
+
+	/* initialize all fields of the region struct */
+	new_region->vbase     = vaddr;
+	new_region->npages    = npages;
+	new_region->readable  = readable;
+	new_region->writeable = writeable;
+	new_region->can_write = writeable;
+	new_region->next      = NULL;
+
+	as->nregions++;
+
+	/* append new_region to as->region_list */
+	struct region *curr_region = as->region_list;
+	if (curr_region != NULL) {
+		struct region *old_head = as->region_list;
+		new_region->next = old_head;
+	}
+	as->region_list = new_region;
 
-        (void)as;
         return 0;
 }
 
-int
-as_complete_load(struct addrspace *as)
-{
+/* make all regions writeable for the purposes of loading into read-only regions initially, e.g. code */
+int as_prepare_load(struct addrspace *as) {
         /*
-         * Write this.
+	 * initial stack pointer will be USERSTACK, see as_define_stack()
+	 * base of user stack will be NUM_STACK_PAGES (16 pages) below this
+	 * this needs to be set before the possiblity of vm_fault() being triggered
+	 * vm_fault() maybe triggered before as_define_stack() is called
+	 * but not before as_prepare_load() is called
          */
+	int ret = as_define_region(as, USERSTACK - PAGE_SIZE * NUM_STACK_PAGES, PAGE_SIZE * NUM_STACK_PAGES, true, true, false);
+	if (ret) return ret;
 
-        (void)as;
-        return 0;
+	/* set writable flag to true for all regions temporarily */
+	for (struct region *curr = as->region_list; curr != NULL; curr = curr->next) {
+		curr->writeable = true;
 }
 
-int
-as_define_stack(struct addrspace *as, vaddr_t *stackptr)
-{
-        /*
-         * Write this.
-         */
+	return 0;
+}
 
-        (void)as;
+/* reset real permissions in all regions */
+int as_complete_load(struct addrspace *as) {
+	for (struct region *curr = as->region_list; curr != NULL; curr = curr->next) {
+		/* reset write flag to real write flag */
+		curr->writeable = curr->can_write;
 
-        /* Initial user-level stack pointer */
-        *stackptr = USERSTACK;
+		/* flip the dirty bit in the ptable entry for all entires for pages in this region */
+		if (!curr->can_write) {
+			for (vaddr_t addr = curr->vbase; addr != curr->vbase + curr->npages * PAGE_SIZE; addr += PAGE_SIZE) {
+				make_page_read_only(addr);
+			}
+		}
+	}
 
+	as_activate(); /* flush the tlb since some entries are now read-only */
         return 0;
 }
 
+/* initial user-level stack pointer */
+int as_define_stack(struct addrspace *as, vaddr_t *stackptr) {
+	(void) as; /* unused */
+	*stackptr = USERSTACK;
+	return 0;
+}
diff --unidirectional-new-file -EdbwBr -u -X /home/cs3231/assigns/asst3/src/../diffex /home/cs3231/assigns/asst3/src/kern/vm/frametable.c /import/ravel/3/z5059988/cs3231/asst3-src/kern/vm/frametable.c
--- kern/vm/frametable.c	2018-02-24 13:25:22.000000000 +1100
+++ kern/vm/frametable.c	2018-06-03 17:46:35.211119049 +1000
@@ -5,42 +5,45 @@
 #include <addrspace.h>
 #include <vm.h>
 
-/* Place your frametable data-structures here 
- * You probably also want to write a frametable initialisation
- * function and call it from vm_bootstrap
- */
-
+ftable_entry fhead;
+struct frame_table_entry *ftable;
 static struct spinlock stealmem_lock = SPINLOCK_INITIALIZER;
 
-/* Note that this function returns a VIRTUAL address, not a physical 
- * address
- * WARNING: this function gets called very early, before
- * vm_bootstrap().  You may wish to modify main.c to call your
- * frame table initialisation function, or check to see if the
- * frame table has been initialised and call ram_stealmem() otherwise.
- */
-
-vaddr_t alloc_kpages(unsigned int npages)
-{
         /*
-         * IMPLEMENT ME.  You should replace this code with a proper
-         *                implementation.
+ * Return a kernel virtual address, not a physical
+ * address for some newly-allocated frame.
+ * Only one frame can be allocated at a time.
          */
-
+vaddr_t alloc_kpages(unsigned int npages) {
+	if (npages != 1) return 0;
         paddr_t addr;
-
         spinlock_acquire(&stealmem_lock);
+	if (ftable == 0) {
+		/* use ram_stealmem if ftable isn't initialised */
         addr = ram_stealmem(npages);
+	} else {
+		if (fhead == NULL) {
         spinlock_release(&stealmem_lock);
+			return 0; /* out of frames */
+		}
 
-        if(addr == 0)
-                return 0;
-
+		/* fhead->addr is stored as 20 bits so we need to shift it to form a paddr_t */
+		addr = (paddr_t)(fhead->addr << PAGE_BITS);
+		fhead = fhead->next;
+	}
+	spinlock_release(&stealmem_lock);
+	if (addr == 0) return 0;
         return PADDR_TO_KVADDR(addr);
 }
 
-void free_kpages(vaddr_t addr)
-{
-        (void) addr;
+/*
+ * Free the page at addr and set it to be the new head, pointing next to the old head.
+ * The addr must be a kernel virtual address, not a physical address.
+ */
+void free_kpages(vaddr_t addr) {
+	spinlock_acquire(&stealmem_lock);
+	ftable_entry old_head = fhead;
+	fhead = (ftable_entry) (KVADDR_TO_PADDR(addr) / PAGE_SIZE + ftable);
+	fhead->next = old_head;
+	spinlock_release(&stealmem_lock);
 }
-
diff --unidirectional-new-file -EdbwBr -u -X /home/cs3231/assigns/asst3/src/../diffex /home/cs3231/assigns/asst3/src/kern/vm/vm.c /import/ravel/3/z5059988/cs3231/asst3-src/kern/vm/vm.c
--- kern/vm/vm.c	2018-02-24 13:25:22.000000000 +1100
+++ kern/vm/vm.c	2018-06-03 17:46:35.211119049 +1000
@@ -3,38 +3,354 @@
 #include <lib.h>
 #include <thread.h>
 #include <addrspace.h>
+#include <proc.h>
+#include <current.h>
+#include <spl.h>
 #include <vm.h>
 #include <machine/tlb.h>
+#include <synch.h>
 
-/* Place your page table functions here */
+ftable_entry fhead = 0; /* pntr to first free entry in frame table */
+uint32_t total_hpt_pages = 0; /* total pages in the hpt */
+struct frame_table_entry *ftable = 0;
+struct page_table_entry *ptable = 0;
+struct lock *hpt_lock;
 
+/*
+ * Initialise the frame table and hashed page table.
+ */
+static void init_tables(paddr_t phys_size, paddr_t first_free) {
+	vaddr_t kernel_top = PADDR_TO_KVADDR(first_free); /* top of kernel */
+	uint32_t total_frames = phys_size / PAGE_SIZE; /* total number of frames in physical memory */
+	total_hpt_pages = total_frames * 2; /* size hpt to twice as many physical frames */
 
-void vm_bootstrap(void)
-{
-        /* Initialise VM sub-system.  You probably want to initialise your 
-           frame table here as well.
+	/* total number of bytes required to store the frame table and HPT */
+	uint32_t ft_size  = total_frames * sizeof(struct frame_table_entry);
+	uint32_t hpt_size = total_hpt_pages * sizeof(struct page_table_entry);
+
+	/* total number of frames used by the kernel, frame table and HPT */
+	uint32_t kern_frames = first_free / PAGE_SIZE;
+	uint32_t ft_frames   = ft_size    / PAGE_SIZE;
+	uint32_t hpt_frames  = hpt_size   / PAGE_SIZE;
+
+	/* place frame table right after kernel */
+	ftable = (ftable_entry) kernel_top;
+
+	/* initialise frame table - set physical frame number (addr) and next pointer */
+	for (uint32_t i = 0; i < total_frames; ++i) {
+		ftable[i].addr = i;
+
+		/* point to next frame; last frame has no next */
+		ftable[i].next = (i != total_frames - 1) ? &ftable[i + 1] : NULL;
+	}
+
+	/* place page table right after kernel and frame table */
+	ptable = (ptable_entry) (kernel_top + ft_size);
+
+	/* initialise page table */
+	for (uint32_t i = 0; i < total_hpt_pages; ++i) {
+		ptable[i].pid = 0;
+		ptable[i].entryhi = 0;
+		ptable[i].entrylo = 0;
+		ptable[i].next = NULL;
+	}
+
+	/* set the first free frame to be directly after the frames used by the kernel, frame table and HPT */
+	fhead = &ftable[kern_frames + ft_frames + hpt_frames];
+}
+
+/*
+ * Initialise the VM system.
         */
+void vm_bootstrap(void) {
+	/* create hpt lock */
+	hpt_lock = lock_create("hpt_lock");
+	KASSERT(hpt_lock != NULL);
+
+	paddr_t phys_size = ram_getsize(); /* must be called first, since ram_getfirstfree() invalidates it */
+	paddr_t first_free = ram_getfirstfree();
+
+	/* initialise frame table and hashed page table */
+	init_tables(phys_size, first_free);
 }
 
-int
-vm_fault(int faulttype, vaddr_t faultaddress)
-{
-        (void) faulttype;
-        (void) faultaddress;
+/*
+ * Zero-fill a region of memory. The paddr must be a kernel virtual address.
+ */
+static void zero_region(vaddr_t paddr, unsigned npages) {
+	KASSERT(paddr != 0 && npages != 0);
+	bzero((void *)paddr, npages * PAGE_SIZE);
+}
 
-        panic("vm_fault hasn't been written yet\n");
+/*
+ * Insert an entry into the ptable for the given vaddr.
+ * Call alloc_kpages() to acquire a frame for the page.
+ * Set the dirty bit as needed for write permissions.
+ */
+int insert_ptable_entry(struct addrspace *as, vaddr_t vaddr, int writeable, bool write_tlb) {
+	KASSERT(as != NULL && vaddr != 0);
+	vaddr &= PAGE_FRAME;
+	vaddr_t paddr = alloc_kpages(1);
+	KASSERT(paddr % PAGE_SIZE == 0);
+	if (paddr == 0) return ENOMEM; /* out of frames */
 
-        return EFAULT;
+	/* check we currently hold the hpt lock - if not acquire it */
+	bool release_lock = !lock_do_i_hold(hpt_lock);
+	if (release_lock) lock_acquire(hpt_lock);
+
+	/* find a free slot to insert the ptable entry */
+	uint32_t index = hpt_hash(as, vaddr);
+	uint32_t candidate = index;
+
+	/* check if the hash slot already has a valid HPT entry */
+	ptable_entry entry = &ptable[candidate];
+	bool initial_collision = (entry->entrylo & TLBLO_VALID);
+
+	/* do a linear scan until the 1st free slot is found */
+	while (entry->entrylo & TLBLO_VALID) {
+		candidate = (candidate + 1) % total_hpt_pages;
+
+		/* check if we looped back around - very unlikely, should run out of frames first */
+		if (candidate == index) {
+			free_kpages(paddr);
+			if (release_lock) lock_release(hpt_lock);
+			return ENOMEM; /* out of pages */
+		}
+		entry = &ptable[candidate];
 }
 
+	/* zero-fill the frame */
+	zero_region(paddr, 1);
+
 /*
- *
- * SMP-specific functions.  Unused in our configuration.
+	 * if there was an collision, find the end of the collision chain
+	 * and set the last entry in the chain to point to the new entry
  */
+	ptable_entry curr = &ptable[index];
+	if (initial_collision) {
+		while (curr->next != NULL) curr = curr->next;
+		curr->next = entry;
+	}
 
-void
-vm_tlbshootdown(const struct tlbshootdown *ts)
-{
+	/* set ptable entry values */
+	entry->pid = (uint32_t) as;
+	entry->entryhi = vaddr;
+	entry->entrylo = KVADDR_TO_PADDR(paddr) | TLBLO_VALID;
+	entry->next = NULL;
+
+	/* set TLBLO_DIRTY in entrylo if writeable */
+	if (writeable) entry->entrylo |= TLBLO_DIRTY;
+
+	/* write new ptable entry to tlb */
+	if (write_tlb) {
+		int spl = splhigh();
+		tlb_random(entry->entryhi, entry->entrylo);
+		splx(spl);
+	}
+
+	if (release_lock) lock_release(hpt_lock);
+	return 0;
+}
+
+/*
+ * Unset the dirty bit in the ptable entry for the given vaddr.
+ */
+void make_page_read_only(vaddr_t vaddr) {
+	KASSERT(vaddr != 0);
+	vaddr &= PAGE_FRAME;
+	struct addrspace *as = proc_getas();
+	lock_acquire(hpt_lock);
+
+	/* find ptable entry by traversing ptable using next pntrs to handle collisions */
+	ptable_entry curr = search_ptable(as, vaddr, NULL);
+
+	/* unset dirty bit in entrylo */
+	if (curr != NULL) {
+		paddr_t paddr = curr->entrylo & TLBLO_PPAGE;
+		curr->entrylo = paddr | TLBLO_VALID;
+	}
+
+	lock_release(hpt_lock);
+}
+
+/*
+ * Remove page table entries and free frames associated with a region.
+ */
+void free_region(struct addrspace *as, vaddr_t vaddr, uint32_t npages) {
+	lock_acquire(hpt_lock);
+	for (vaddr_t page = vaddr; page != vaddr + npages * PAGE_SIZE; page += PAGE_SIZE) {
+		ptable_entry prev = NULL;
+		ptable_entry pt = search_ptable(as, page, &prev); /* find ptable entry associated with page */
+		if (pt == NULL) continue; /* no HPT entry found */
+		KASSERT((pt->entryhi & TLBHI_VPAGE) == page);
+		free_kpages(PADDR_TO_KVADDR(pt->entrylo & TLBLO_PPAGE));
+		ptable_entry to_remove = pt;
+
+		if (prev != NULL) {
+			/* update collision chain to "skip" this entry */
+			prev->next = pt->next;
+		} else if (pt->next != NULL) {
+			/* head of chain - move last entry in chain to the start */
+			ptable_entry curr = pt;
+			prev = curr;
+
+			/* get the last entry in the chain */
+			while (curr->next != NULL) {
+				prev = curr;
+				curr = curr->next;
+			}
+
+			/* move entry to head of chain */
+			pt->pid = curr->pid;
+			pt->entryhi = curr->entryhi;
+			pt->entrylo = curr->entrylo;
+			to_remove = curr;
+
+			/* make 2nd-last slot of chain the last slot */
+			prev->next = NULL;
+		}
+
+		/* reset ptable entry */
+		to_remove->pid = 0;
+		to_remove->entryhi = 0;
+		to_remove->entrylo = 0;
+	}
+	lock_release(hpt_lock);
+}
+
+/*
+ * Copy ptable entries from old to new addrspace and allocate new frames.
+ */
+int copy_region(struct region *reg, struct addrspace *old, struct addrspace *newas) {
+	vaddr_t addr = reg->vbase;
+	lock_acquire(hpt_lock);
+	while (addr != reg->vbase + reg->npages * PAGE_SIZE) {
+		/* check an old page table entry exists for the page */
+		ptable_entry old_pt = search_ptable(old, addr, NULL);
+
+		if (old_pt != NULL) {
+			/* insert page table entry for each page in the copied region */
+			int ret = insert_ptable_entry(newas, addr, reg->writeable, false);
+			if (ret) {
+				lock_release(hpt_lock);
+				return ret;
+			}
+
+			/* get ptable entry for new page */
+			ptable_entry new_pt = search_ptable(newas, addr, NULL);
+			if (new_pt == NULL) return ENOMEM; /* this should never happen, very unlikely */
+
+			/* get frame number for old and new frames */
+			vaddr_t old_frame = PADDR_TO_KVADDR(old_pt->entrylo & TLBLO_PPAGE);
+			vaddr_t new_frame = PADDR_TO_KVADDR(new_pt->entrylo & TLBLO_PPAGE);
+
+			/* copy the memory from the old frame to the new frame */
+			memmove((void *) new_frame, (const void *) old_frame, PAGE_SIZE);
+		}
+
+		addr += PAGE_SIZE;
+	}
+	lock_release(hpt_lock);
+	return 0;
+}
+
+/*
+ * Find the ptable entry with the given vaddr and pid.
+ * Begin the search from curr and follow the collision pointers until found.
+ * Requires the page table lock to have been acquired already.
+ * Also sets prev to point to the previous ptable entry in the collision chain.
+ */
+ptable_entry search_ptable(struct addrspace *as, vaddr_t vaddr, ptable_entry *prev) {
+	KASSERT(vaddr != 0);
+	KASSERT((vaddr & PAGE_FRAME) == vaddr);
+	KASSERT(lock_do_i_hold(hpt_lock));
+
+	uint32_t index = hpt_hash(as, vaddr);
+	pid_t pid = (uint32_t) as;
+	ptable_entry curr = &ptable[index];
+	do {
+		/* if vaddr and pid in curr match and it is valid - found */
+		if ((curr->entryhi & TLBHI_VPAGE) == vaddr && pid == curr->pid && (curr->entrylo & TLBLO_VALID)) break;
+		if (prev != NULL) *prev = curr;
+		curr = curr->next;
+	} while (curr != NULL);
+	return curr;
+}
+
+/*
+ * Return an index into the ptable for the given addrspace and addr.
+ */
+uint32_t hpt_hash(struct addrspace *as, vaddr_t addr) {
+	KASSERT(as != NULL && addr != 0);
+	KASSERT((addr & PAGE_FRAME) == addr);
+	uint32_t index;
+	index = (((uint32_t) as) ^ (addr >> PAGE_BITS)) % total_hpt_pages;
+	return index;
+}
+
+/*
+ * Find the ptable entry corresponding to the faultaddress and load into the tlb.
+ */
+int vm_fault(int faulttype, vaddr_t faultaddress) {
+	switch (faulttype) {
+	case VM_FAULT_READONLY:
+		return EFAULT; /* attempt to write to read-only page */
+	case VM_FAULT_READ:
+	case VM_FAULT_WRITE:
+		break;
+	default:
+		return EINVAL; /* unknown faulttype */
+	}
+	if (curproc == NULL) return EFAULT;
+	struct addrspace *as = proc_getas();
+	if (as == NULL) return EFAULT;
+
+	/* assert that the address space has been set up properly */
+	KASSERT(as->region_list != NULL);
+	struct region *curr_region = as->region_list;
+	uint32_t nregions = 0;
+	struct region *region_found = NULL;
+	faultaddress &= PAGE_FRAME;
+
+	while (curr_region != NULL) {
+		/* assert that region is set up correctly */
+		KASSERT(curr_region->vbase != 0);
+		KASSERT(curr_region->npages != 0);
+		KASSERT((curr_region->vbase & PAGE_FRAME) == curr_region->vbase);
+
+		/* check if vaddr is in a valid region */
+		if (!region_found && faultaddress >= curr_region->vbase && faultaddress < curr_region->vbase + curr_region->npages * PAGE_SIZE) {
+			region_found = curr_region;
+		}
+		curr_region = curr_region->next;
+		nregions++;
+	}
+	if (region_found == NULL) return EFAULT;
+	KASSERT(as->nregions == nregions);
+	lock_acquire(hpt_lock);
+
+	/* find ptable entry by traversing ptable using next pntrs to handle collisions */
+	ptable_entry curr = search_ptable(as, faultaddress, NULL);
+
+	if (curr == NULL) {
+		lock_release(hpt_lock);
+
+		/* lazy page/frame allocation */
+		int ret = insert_ptable_entry(as, faultaddress, region_found->writeable, true);
+		if (ret) return ret;
+	} else {
+		int spl = splhigh();
+		tlb_random(curr->entryhi, curr->entrylo);
+		splx(spl);
+		lock_release(hpt_lock);
+	}
+	return 0;
+}
+
+/*
+ * SMP-specific functions. Unused in our configuration.
+ */
+void vm_tlbshootdown(const struct tlbshootdown *ts) {
         (void)ts;
         panic("vm tried to do tlb shootdown?!\n");
 }
